{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Deploy a model as an online endpoint\n",
        "\n",
        "Learn to deploy a model to an online endpoint, using Azure Machine Learning Python SDK v2.\n",
        "\n",
        "In this tutorial, we use a model trained to predict the likelihood of defaulting on a credit card payment. The goal is to deploy this model and show its use.\n",
        "\n",
        "The steps you'll take are:\n",
        "\n",
        "> * Create an endpoint and a first deployment of registered model\n",
        "> * Deploy a trial run\n",
        "> * Manually send test data to the deployment of registered model\n",
        "> * Get details of the deployment\n",
        "\n",
        "(Advanced)\n",
        "> * Create a second deployment\n",
        "> * Manually scale the second deployment\n",
        "> * Update allocation of production traffic between both deployments\n",
        "> * Get details of the second deployment\n",
        "> * Roll out the new deployment and delete the first one"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "\n",
        "1. Open in studio and select a compute instance.\n",
        "    * If you opened this notebook from Azure Machine Learning studio, you need a compute instance to run the code. If you don't have a compute instance, select **Create compute** on the toolbar to first create one.  You can use all the default settings.  \n",
        "    \n",
        "    ![Create compute](./media/create-compute.png)\n",
        "    \n",
        "    * If you're seeing this notebook elsewhere, complete [Create resources you need to get started](https://docs.microsoft.com/azure/machine-learning/quickstart-create-resources) to create an Azure Machine Learning workspace and a compute instance.\n",
        "    \n",
        "1. View your VM quota and ensure you have enough quota available to create online deployments. In this tutorial, you will need at least 8 cores of `STANDARD_DS3_v2` and 12 cores of `STANDARD_F4s_v2`. To view your VM quota usage and request quota increases, see [Manage resource quotas](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas#view-your-usage-and-quotas-in-the-azure-portal).\n",
        "\n",
        "## Set your kernel\n",
        "\n",
        "* If your compute instance is stopped, start it now.  \n",
        "        \n",
        "    ![Start compute](./media/start-compute.png)\n",
        "\n",
        "* Once your compute instance is running, make sure the that the kernel, found on the top right, is `Python 3.10 - SDK v2`.  If not, use the dropdown to select this kernel.\n",
        "\n",
        "    ![Set the kernel](./media/set-kernel.png)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create handle to workspace\n",
        "\n",
        "Before we dive in the code, you need a way to reference your workspace. You'll create `ml_client` for a handle to the workspace.  You'll then use `ml_client` to manage resources and jobs.\n",
        "\n",
        "In the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find these values:\n",
        "\n",
        "1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
        "1. Copy the value for workspace, resource group and subscription ID into the code.\n",
        "1. You'll need to copy one value, close the area and paste, then come back for the next one."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential, AzureCliCredential, ManagedIdentityCredential\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from dotenv import dotenv_values\n",
        "import requests \n",
        "import os\n",
        "\n",
        "config = dotenv_values(\".env\") \n",
        "subscription_id =config['SUBSCRIPTION_ID']\n",
        "resource_group = config['RESOURCE_GP']\n",
        "workspace = config['WORKSPACE']\n",
        "\n",
        "os.environ[\"AZURE_CLIENT_ID\"] = config[\"AZURE_CLIENT_ID\"]\n",
        "os.environ[\"AZURE_CLIENT_SECRET\"] = config[\"AZURE_CLIENT_SECRET\"]\n",
        "os.environ[\"AZURE_TENANT_ID\"] = config[\"AZURE_TENANT_ID\"]\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "# Check if given credential can get token successfully.\n",
        "credential.get_token(\"https://management.azure.com/.default\")\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group_name=resource_group,\n",
        "    workspace_name=workspace,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1686900778950
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "> [!NOTE]\n",
        "> Creating `MLClient` will not connect to the workspace. The client initialization is lazy and will wait for the first time it needs to make a call (this will happen in the next code cell).\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confirm that the model is registered\n",
        "\n",
        "You can check the **Models** page in [Azure Machine Learning studio](https://ml.azure.com/) to identify the latest version of your registered model.\n",
        "\n",
        "![View model](./media/registered-model-in-studio.png)\n",
        "\n",
        "Alternatively, the code below will retrieve the latest version number for you to use."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"credit_defaults_model\"\n",
        "\n",
        "# Let's pick the latest version of the model\n",
        "latest_model_version = max(\n",
        "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
        ")\n",
        "\n",
        "print(latest_model_version)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "9\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1686900868347
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have a reference to the registered model, you can create an endpoint and deployment. The next section will briefly cover some key details about these topics."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Endpoints and deployments\n",
        "\n",
        "After you train a machine learning model, you need to deploy it so that others can use it for inferencing. For this purpose, Azure Machine Learning allows you to create **endpoints** and add **deployments** to them.\n",
        "\n",
        "An **endpoint**, in this context, is an HTTPS path that provides an interface for clients to send requests (input data) to a trained model and receive the inferencing (scoring) results back from the model. An endpoint provides:\n",
        "\n",
        "- Authentication using \"key or token\" based auth \n",
        "- [TLS(SSL)](https://simple.wikipedia.org/wiki/Transport_Layer_Security) termination\n",
        "- A stable scoring URI (endpoint-name.region.inference.ml.azure.com)\n",
        "\n",
        "\n",
        "A **deployment** is a set of resources required for hosting the model that does the actual inferencing. \n",
        "\n",
        "A single endpoint can contain multiple deployments. Endpoints and deployments are independent Azure Resource Manager resources that appear in the Azure portal.\n",
        "\n",
        "Azure Machine Learning allows you to implement [online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints#what-are-online-endpoints) for real-time inferencing on client data, and [batch endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints#what-are-batch-endpoints) for inferencing on large volumes of data over a period of time. \n",
        "\n",
        "In this tutorial, we'll walk you through the steps of implementing a _managed online endpoint_. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way that frees you from the overhead of setting up and managing the underlying deployment infrastructure."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an online endpoint\n",
        "\n",
        "Now that you have a registered model, it's time to create your online endpoint. The endpoint name needs to be unique in the entire Azure region. For this tutorial, you'll create a unique name using a universally unique identifier [`UUID`](https://en.wikipedia.org/wiki/Universally_unique_identifier). For more information on the endpoint naming rules, see [managed online endpoint limits](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "# Create a unique name for the endpoint\n",
        "online_endpoint_name = \"credit-endpoint-\" + str(uuid.uuid4())[:8]"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686900871998
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll define the endpoint, using the `ManagedOnlineEndpoint` class.\n",
        "\n",
        "\n",
        "\n",
        "> [!TIP]\n",
        "> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](https://learn.microsoft.com/azure/machine-learning/how-to-authenticate-online-endpoint).\n",
        "> * Optionally, you can add a description and tags to your endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
        "\n",
        "# define an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is an online endpoint\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\n",
        "        \"training_dataset\": \"credit_defaults\",\n",
        "    },\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686900872995
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `MLClient` created earlier, we'll now create the endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues.\n",
        "\n",
        "> [!NOTE]\n",
        "> Expect the endpoint creation to take approximately 2 minutes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create the online endpoint\n",
        "# expect the endpoint to take approximately 2 minutes.\n",
        "\n",
        "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686900972007
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've created the endpoint, you can retrieve it as follows:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "print(\n",
        "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Endpoint \"credit-endpoint-c8c5b818\" with provisioning state \"Succeeded\" is retrieved\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686900972162
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a deployment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Understanding online deployments\n",
        "\n",
        "The key aspects of a deployment include:\n",
        "\n",
        "- `name` - Name of the deployment.\n",
        "- `endpoint_name` - Name of the endpoint that will contain the deployment.\n",
        "- `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n",
        "- `environment` - The environment to use for the deployment (or to run the model). This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification. The environment can be a Docker image with Conda dependencies or a Dockerfile.\n",
        "- `code_configuration` - the configuration for the source code and scoring script.\n",
        "    - `path`- Path to the source code directory for scoring the model.\n",
        "    - `scoring_script` - Relative path to the scoring file in the source code directory. This script executes the model on a given input request. For an example of a scoring script, see [Understand the scoring script](https://learn.microsoft.com/azure/machine-learning/how-to-deploy-online-endpoints#understand-the-scoring-script) in the \"Deploy an ML model with an online endpoint\" article.\n",
        "- `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](https://learn.microsoft.com/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).\n",
        "- `instance_count` - The number of instances to use for the deployment.\n",
        "    \n",
        "### Deployment using an MLflow model\n",
        "\n",
        "Azure Machine Learning supports no-code deployment of a model created and logged with MLflow. This means that you don't have to provide a scoring script or an environment during model deployment, as the scoring script and environment are automatically generated when training an MLflow model. If you were using a custom model, though, you'd have to specify the environment and scoring script during deployment.\n",
        "\n",
        "> [!IMPORTANT]\n",
        "> If you typically deploy models using scoring scripts and custom environments and want to achieve the same functionality using MLflow models, we recommend reading [Using MLflow models for no-code deployment](https://learn.microsoft.com/azure/machine-learning/how-to-deploy-mlflow-models)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy the model to the endpoint\n",
        "\n",
        "You'll begin by creating a single deployment that handles 100% of the incoming traffic. We use *blue* as the deployment name. To create the deployment for our endpoint, we'll use the `ManagedOnlineDeployment` class.\n",
        "\n",
        "> [!NOTE]\n",
        "> No need to specify an environment or scoring script as the model to deploy is an MLflow model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import ManagedOnlineDeployment\n",
        "\n",
        "# Choose the latest version of our registered model for deployment\n",
        "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
        "\n",
        "# define an online deployment\n",
        "# if you run into an out of quota error, change the instance_type to a comparable VM that is available.\n",
        "# Learn more on https://azure.microsoft.com/en-us/pricing/details/machine-learning/.\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=\"Standard_DS3_v2\",\n",
        "    instance_count=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686900972320
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `MLClient` created earlier, we'll now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create the online deployment\n",
        "blue_deployment = ml_client.online_deployments.begin_create_or_update(\n",
        "    blue_deployment\n",
        ").result()\n",
        "\n",
        "# blue deployment takes 100% traffic\n",
        "# expect the deployment to take approximately 8 to 10 minutes.\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint credit-endpoint-c8c5b818 exists\ndata_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "........................................................................................................................................................................................................................................................................................................................................................................................................"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create the online deployment\u001b[39;00m\n\u001b[1;32m      2\u001b[0m blue_deployment \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_deployments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblue_deployment\u001b[49m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# blue deployment takes 100% traffic\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# expect the deployment to take approximately 8 to 10 minutes.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m endpoint\u001b[38;5;241m.\u001b[39mtraffic \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m}\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/_poller.py:230\u001b[0m, in \u001b[0;36mLROPoller.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PollingReturnType:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the result of the long running operation, or\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    the result available after the specified timeout.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    :raises ~azure.core.exceptions.HttpResponseError: Server problem with the query.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_polling_method\u001b[38;5;241m.\u001b[39mresource()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/_poller.py:245\u001b[0m, in \u001b[0;36mLROPoller.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# Let's handle possible None in forgiveness here\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/8165\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686903027188
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the status of the endpoint\n",
        "You can check the status of the endpoint to see whether the model was deployed without error:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# return an object that contains metadata for the endpoint\n",
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "# print a selection of the endpoint's metadata\n",
        "print(\n",
        "    f\"Name: {endpoint.name}\\nStatus: {endpoint.provisioning_state}\\nDescription: {endpoint.description}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686903027233
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# existing traffic details\n",
        "print(endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(endpoint.scoring_uri)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686903027242
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the endpoint with sample data\n",
        "\n",
        "Now that the model is deployed to the endpoint, you can run inference with it. Let's create a sample request file following the design expected in the run method in the scoring script."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a directory to store the sample request file.\n",
        "deploy_dir = \"./deploy\"\n",
        "os.makedirs(deploy_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1686837492614
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, create the file in the deploy directory. The cell below uses IPython magic to write the file into the directory you just created."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {deploy_dir}/sample-request.json\n",
        "{\n",
        "  \"input_data\": {\n",
        "    \"columns\": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n",
        "    \"index\": [0, 1],\n",
        "    \"data\": [\n",
        "            [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0],\n",
        "            [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8]\n",
        "            ]\n",
        "                }\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./deploy/sample-request.json\n"
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `MLClient` created earlier, we'll get a handle to the endpoint. The endpoint can be invoked using the `invoke` command with the following parameters:\n",
        "\n",
        "* `endpoint_name` - Name of the endpoint\n",
        "* `request_file` - File with request data\n",
        "* `deployment_name` - Name of the specific deployment to test in an endpoint\n",
        "\n",
        "We'll test the blue deployment with the sample data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test the blue deployment with the sample data\n",
        "ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    deployment_name=\"blue\",\n",
        "    request_file=\"./deploy/sample-request.json\",\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "'[1, 0]'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1686837502580
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Get logs of the deployment\n",
        "Check the logs to see whether the endpoint/deployment were invoked successfully\n",
        "If you face errors, see [Troubleshooting online endpoints deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints?tabs=cli)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "logs = ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
        ")\n",
        "print(logs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Instance status:\nSystemSetup: Succeeded\nUserContainerImagePull: Succeeded\nModelDownload: Succeeded\nUserContainerStart: Succeeded\n\nContainer events:\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:43:23.629402Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:43:32.811553Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:43:33.629538Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:43:42.81134Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:43:43.629388Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:43:52.811418Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:43:53.629499Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:02.811363Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:03.629355Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:12.811421Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:13.62942Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:22.811307Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:23.629384Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:34.837603Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:34.837692Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:42.8115Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:43.629584Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:52.8113Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2023-06-15T13:44:53.629592Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\nKind: Pod, Name: ContainerReady, Type: Normal, Time: 2023-06-15T13:45:04.952567628Z, Message: Container is ready\n\nContainer logs:\n2023-06-15 13:54:32,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:54:32 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:54:33,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:54:33 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:54:42,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:54:42 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:54:43,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:54:43 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:54:52,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:54:52 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:54:53,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:54:53 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:02,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:02 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:03,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:03 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:12,813 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:12 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:13,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:13 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:22,813 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:22 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:23,631 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:23 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:32,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:32 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:33,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:33 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:42,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:42 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:43,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:43 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:52,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:52 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:55:53,631 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:55:53 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:02,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:02 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:03,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:03 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:12,813 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:12 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:13,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:13 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:22,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:22 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:23,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:23 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:32,813 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:32 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:33,631 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:33 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:42,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:42 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:43,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:43 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:52,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:52 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:56:53,631 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:56:53 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:02,813 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:02 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:03,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:03 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:12,813 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:12 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:13,631 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:13 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:22,813 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:22 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:23,631 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:23 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:32,813 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:32 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:33,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:33 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:42,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:42 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:43,631 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:43 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:52,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:52 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:57:53,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:57:53 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:58:02,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:58:02 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:58:03,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:58:03 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:58:12,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:58:12 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:58:13,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:58:13 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:58:22,508 I [695] azmlinfsrv - POST /score 200 8.795ms 6\n2023-06-15 13:58:22,508 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:58:22 +0000] \"POST /score HTTP/1.0\" 200 6 \"-\" \"azure-ai-ml/1.5.0 azsdk-python-core/1.26.4 Python/3.10.10 (Linux-5.15.0-1035-azure-x86_64-with-glibc2.31)\"\n2023-06-15 13:58:22,812 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:58:22 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n2023-06-15 13:58:23,630 I [695] gunicorn.access - 127.0.0.1 - - [15/Jun/2023:13:58:23 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.18\"\n\n\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1686837503964
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a second deployment \n",
        "Deploy the model as a second deployment called `green`. In practice, you can create several deployments and compare their performance. These deployments could use a different version of the same model, a completely different model, or a more powerful compute instance. In our example, you'll deploy the same model version using a more powerful compute instance that could potentially improve performance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# picking the model to deploy. Here we use the latest version of our registered model\n",
        "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
        "\n",
        "# define an online deployment using a more powerful instance type\n",
        "# if you run into an out of quota error, change the instance_type to a comparable VM that is available.\n",
        "# Learn more on https://azure.microsoft.com/en-us/pricing/details/machine-learning/.\n",
        "green_deployment = ManagedOnlineDeployment(\n",
        "    name=\"green\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=\"Standard_F4s_v2\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "# create the online deployment\n",
        "# expect the deployment to take approximately 8 to 10 minutes\n",
        "green_deployment = ml_client.online_deployments.begin_create_or_update(\n",
        "    green_deployment\n",
        ").result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint credit-endpoint-f1e2ef8b exists\ndata_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "................................................................................................................."
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1686842440639
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scale deployment to handle more traffic\n",
        "\n",
        "Using the `MLClient` created earlier, we'll get a handle to the `green` deployment. The deployment can be scaled by increasing or decreasing the `instance_count`.\n",
        "\n",
        "In the following code, you'll increase the VM instance manually. However, note that it is also possible to autoscale online endpoints. Autoscale automatically runs the right amount of resources to handle the load on your application. Managed online endpoints support autoscaling through integration with the Azure monitor autoscale feature. To configure autoscaling, see [autoscale online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-autoscale-endpoints?tabs=python)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# update definition of the deployment\n",
        "green_deployment.instance_count = 2\n",
        "\n",
        "# update the deployment\n",
        "# expect the deployment to take approximately 8 to 10 minutes\n",
        "ml_client.online_deployments.begin_create_or_update(green_deployment).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint credit-endpoint-f1e2ef8b exists\ndata_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "................................................................................................................................................"
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "ManagedOnlineDeployment({'private_network_connection': False, 'provisioning_state': 'Succeeded', 'data_collector': None, 'endpoint_name': 'credit-endpoint-f1e2ef8b', 'type': 'Managed', 'name': 'green', 'description': None, 'tags': {}, 'properties': {'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/9017d57d-c4df-480d-b92d-7aea2266b0f0/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/od:d7044285-39e5-4204-8c94-800032325d74:ce1790dc-8297-43e1-95d4-ada636a1fc6e?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/9017d57d-c4df-480d-b92d-7aea2266b0f0/resourceGroups/azure-mlops-demo/providers/Microsoft.MachineLearningServices/workspaces/demo-ws/onlineEndpoints/credit-endpoint-f1e2ef8b/deployments/green', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/lc-cpu/code/Users/lingzhen.chen/workshop-demo/get-started-notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f2abc20b400>, 'model': '/subscriptions/9017d57d-c4df-480d-b92d-7aea2266b0f0/resourceGroups/azure-mlops-demo/providers/Microsoft.MachineLearningServices/workspaces/demo-ws/models/credit-default-model/versions/1', 'code_configuration': None, 'environment': None, 'environment_variables': {}, 'app_insights_enabled': False, 'scale_settings': <azure.ai.ml.entities._deployment.scale_settings.DefaultScaleSettings object at 0x7f2abc20ab60>, 'request_settings': <azure.ai.ml.entities._deployment.deployment_settings.OnlineRequestSettings object at 0x7f2abc20b2e0>, 'liveness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f2abc20ac80>, 'readiness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f2abc20aec0>, 'instance_count': 2, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'Standard_F4s_v2', 'egress_public_network_access': 'Enabled'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1686843182843
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update traffic allocation for deployments\n",
        "You can split production traffic between deployments. You may first want to test the `green` deployment with sample data, just like you did for the `blue` deployment. Once you've tested your green deployment, allocate a small percentage of traffic to it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint.traffic = {\"blue\": 80, \"green\": 20}\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\nReadonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://credit-endpoint-f1e2ef8b.eastus.inference.ml.azure.com/score', 'openapi_uri': 'https://credit-endpoint-f1e2ef8b.eastus.inference.ml.azure.com/swagger.json', 'name': 'credit-endpoint-f1e2ef8b', 'description': 'this is an online endpoint', 'tags': {'training_dataset': 'credit_defaults'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/9017d57d-c4df-480d-b92d-7aea2266b0f0/resourcegroups/azure-mlops-demo/providers/microsoft.machinelearningservices/workspaces/demo-ws/onlineendpoints/credit-endpoint-f1e2ef8b', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/9017d57d-c4df-480d-b92d-7aea2266b0f0/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oe:d7044285-39e5-4204-8c94-800032325d74:f52c76bc-7b49-416c-a25e-b400158bd8cd?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/9017d57d-c4df-480d-b92d-7aea2266b0f0/resourceGroups/azure-mlops-demo/providers/Microsoft.MachineLearningServices/workspaces/demo-ws/onlineEndpoints/credit-endpoint-f1e2ef8b', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/lc-cpu/code/Users/lingzhen.chen/workshop-demo/get-started-notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f2abc20bb50>, 'auth_mode': 'key', 'location': 'eastus', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7f2abc20ad70>, 'traffic': {'blue': 80, 'green': 20}, 'mirror_traffic': {}, 'kind': 'Managed'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1686857337155
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test traffic allocation by invoking the endpoint several times:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# You can invoke the endpoint several times\n",
        "for i in range(30):\n",
        "    ml_client.online_endpoints.invoke(\n",
        "        endpoint_name=online_endpoint_name,\n",
        "        request_file=\"./deploy/sample-request.json\",\n",
        "    )"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ServiceResponseError",
          "evalue": "HTTPSConnectionPool(host='credit-endpoint-f1e2ef8b.eastus.inference.ml.azure.com', port=443): Read timed out. (read timeout=300)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mServiceResponseError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# You can invoke the endpoint several times\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_endpoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monline_endpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./deploy/sample-request.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_online_endpoint_operations.py:356\u001b[0m, in \u001b[0;36mOnlineEndpointOperations.invoke\u001b[0;34m(self, endpoint_name, request_file, deployment_name, input_data, params_override, local, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deployment_name:\n\u001b[1;32m    354\u001b[0m     headers[EndpointInvokeFields\u001b[38;5;241m.\u001b[39mMODEL_DEPLOYMENT] \u001b[38;5;241m=\u001b[39m deployment_name\n\u001b[0;32m--> 356\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m validate_response(response)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_utils/_http_utils.py:63\u001b[0m, in \u001b[0;36m_request_function.<locals>._.<locals>.decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"A function that sends an HTTP request and returns the response.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mAccepts the same parameters as azure.core.rest.HttpRequest, except for the method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m:return HttpResponse:\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m request \u001b[38;5;241m=\u001b[39m HttpRequest(\n\u001b[1;32m     53\u001b[0m     f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     files\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhttp_response\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/_base.py:202\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[38;5;241m=\u001b[39m PipelineRequest(request, context)\n\u001b[1;32m    201\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/policies/_redirect.py:156\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    154\u001b[0m redirect_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigure_redirects(request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[0;32m--> 156\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     redirect_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_redirect_location(response)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/policies/_retry.py:470\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    468\u001b[0m                 is_response_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    469\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/policies/_retry.py:448\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    446\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[0;32m--> 448\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry(retry_settings, response):\n\u001b[1;32m    450\u001b[0m     retry_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincrement(retry_settings, response\u001b[38;5;241m=\u001b[39mresponse)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/_base.py:70\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     68\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/_base.py:101\u001b[0m, in \u001b[0;36m_TransportRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124;03m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    :param request: The PipelineRequest object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    :rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PipelineResponse(\n\u001b[1;32m    100\u001b[0m         request\u001b[38;5;241m.\u001b[39mhttp_request,\n\u001b[0;32m--> 101\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    102\u001b[0m         context\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mcontext,\n\u001b[1;32m    103\u001b[0m     )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/transport/_requests_basic.py:364\u001b[0m, in \u001b[0;36mRequestsTransport.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     error \u001b[38;5;241m=\u001b[39m ServiceRequestError(err, error\u001b[38;5;241m=\u001b[39merr)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_rest(request):\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_requests_basic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RestRequestsTransportResponse\n",
            "\u001b[0;31mServiceResponseError\u001b[0m: HTTPSConnectionPool(host='credit-endpoint-f1e2ef8b.eastus.inference.ml.azure.com', port=443): Read timed out. (read timeout=300)"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1686857639689
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show logs from the `green` deployment to check that there were incoming requests and the model was scored successfully. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "logs = ml_client.online_deployments.get_logs(\n",
        "    name=\"green\", endpoint_name=online_endpoint_name, lines=50\n",
        ")\n",
        "print(logs)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686857639746
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View metrics using Azure Monitor\n",
        "You can view various metrics (request numbers, request latency, network bytes, CPU/GPU/Disk/Memory utilization, and more) for an online endpoint and its deployments by following links from the endpoint's **Details** page in the studio. Following these links will take you to the exact metrics page in the Azure portal for the endpoint or deployment.\n",
        "\n",
        "![metrics page 1](./media/deployment-metrics-from-endpoint-details-page.png)\n",
        "\n",
        "\n",
        "If you open the metrics for the online endpoint, you can set up the page to see metrics such as the average request latency as shown in the following figure.\n",
        "\n",
        "![metrics page 2](./media/view-endpoint-metrics-in-azure-portal.png)\n",
        "\n",
        "For more information on how to view online endpoint metrics, see [Monitor online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-monitor-online-endpoints#metrics)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send all traffic to the new deployment\n",
        "Once you're fully satisfied with your `green` deployment, switch all traffic to it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint.traffic = {\"blue\": 0, \"green\": 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete the old deployment\n",
        "Remove the old (blue) deployment:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_deployments.begin_delete(\n",
        "    name=\"blue\", endpoint_name=online_endpoint_name\n",
        ").result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean up resources\n",
        "\n",
        "If you aren't going use the endpoint and deployment after completing this tutorial, you should delete them.\n",
        "\n",
        "> [!NOTE]\n",
        "> Expect the complete deletion to take approximately 20 minutes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_delete(name=online_endpoint_name).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "description": {
      "description": "Learn to deploy a model to an online endpoint, using Azure Machine Learning Python SDK v2."
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}